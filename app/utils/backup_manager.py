"""Система резервного копирования базы данных с использованием psycopg2."""

import os
import json
import gzip
from datetime import datetime
from typing import Dict, Any, Optional, List
from pathlib import Path
import psycopg2
from psycopg2.extras import RealDictCursor
from flask import current_app
import logging

logger = logging.getLogger(__name__)

class BackupManager:
    """Менеджер резервного копирования с использованием psycopg2."""
    
    def __init__(self):
        self.backup_dir = Path('backups')
        self.backup_dir.mkdir(exist_ok=True)
        
        # Получаем настройки БД из конфигурации Flask
        self.db_config = self._get_db_config()
    
    def _get_db_config(self) -> Dict[str, str]:
        """Получение конфигурации базы данных."""
        try:
            # Парсим DATABASE_URL или используем отдельные параметры
            database_url = current_app.config.get('SQLALCHEMY_DATABASE_URI')
            
            if database_url.startswith('postgresql://'):
                # Парсим URL вида postgresql://user:pass@host:port/dbname
                from urllib.parse import urlparse
                parsed = urlparse(database_url)
                
                return {
                    'host': parsed.hostname or 'localhost',
                    'port': parsed.port or 5432,
                    'database': parsed.path.lstrip('/'),
                    'user': parsed.username,
                    'password': parsed.password
                }
            else:
                # Используем отдельные параметры
                return {
                    'host': current_app.config.get('DB_HOST'),
                    'port': current_app.config.get('DB_PORT'),
                    'database': current_app.config.get('DB_NAME'),
                    'user': current_app.config.get('DB_USER'),
                    'password': current_app.config.get('DB_PASSWORD')
                }
        except Exception as e:
            logger.error(f"Error getting DB config: {e}")
            return {}
    
    def create_backup(self, compress: bool = True) -> str:
        """
        Создание резервной копии базы данных.
        
        Args:
            compress: Сжимать ли бэкап
            
        Returns:
            str: Путь к созданному файлу бэкапа
        """
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            backup_filename = f"backup_{timestamp}.sql"
            backup_path = self.backup_dir / backup_filename
            
            logger.info(f"Starting backup creation: {backup_path}")
            
            # Создаем SQL файл с бэкапом
            self._create_sql_backup(backup_path)
            
            # Сжимаем если нужно
            if compress:
                compressed_path = backup_path.with_suffix('.sql.gz')
                self._compress_file(backup_path, compressed_path)
                backup_path.unlink()  # Удаляем несжатый файл
                backup_path = compressed_path
            
            logger.info(f"Backup created successfully: {backup_path}")
            return str(backup_path)
            
        except Exception as e:
            logger.error(f"Backup creation failed: {e}")
            raise Exception(f"Ошибка создания резервной копии: {str(e)}")
    
    def _create_sql_backup(self, backup_path: Path) -> None:
        """Создание SQL файла с бэкапом."""
        try:
            with psycopg2.connect(**self.db_config) as conn:
                with conn.cursor(cursor_factory=RealDictCursor) as cursor:
                    
                    with open(backup_path, 'w', encoding='utf-8') as f:
                        # Заголовок бэкапа
                        f.write(f"-- Backup created at {datetime.now().isoformat()}\n")
                        f.write(f"-- Database: {self.db_config['database']}\n")
                        f.write("-- Generated by psycopg2 backup manager\n\n")
                        
                        # Отключаем проверки
                        f.write("SET session_replication_role = replica;\n\n")
                        
                        # Получаем список всех таблиц
                        tables = self._get_tables(cursor)
                        
                        # Экспортируем каждую таблицу
                        for table in tables:
                            self._export_table(cursor, f, table)
                        
                        # Включаем проверки обратно
                        f.write("SET session_replication_role = DEFAULT;\n")
                        
                        # Создаем индексы
                        self._export_indexes(cursor, f)
                        
                        # Создаем последовательности
                        self._export_sequences(cursor, f)
                        
                        # Создаем ограничения
                        self._export_constraints(cursor, f)
                        
        except Exception as e:
            logger.error(f"SQL backup creation failed: {e}")
            raise
    
    def _get_tables(self, cursor) -> List[str]:
        """Получение списка всех таблиц."""
        cursor.execute("""
            SELECT tablename 
            FROM pg_tables 
            WHERE schemaname = 'public' 
            ORDER BY tablename
        """)
        return [row['tablename'] for row in cursor.fetchall()]
    
    def _export_table(self, cursor, file, table_name: str) -> None:
        """Экспорт одной таблицы."""
        try:
            # Получаем структуру таблицы
            cursor.execute(f"""
                SELECT column_name, data_type, is_nullable, column_default
                FROM information_schema.columns 
                WHERE table_name = '{table_name}' 
                ORDER BY ordinal_position
            """)
            columns = cursor.fetchall()
            
            # Создаем таблицу
            file.write(f"\n-- Table: {table_name}\n")
            file.write(f"DROP TABLE IF EXISTS {table_name} CASCADE;\n")
            
            create_table_sql = f"CREATE TABLE {table_name} (\n"
            column_defs = []
            
            for col in columns:
                col_name = col['column_name']
                col_type = col['data_type']
                nullable = "NULL" if col['is_nullable'] == 'YES' else "NOT NULL"
                default = f"DEFAULT {col['column_default']}" if col['column_default'] else ""
                
                column_def = f"    {col_name} {col_type} {nullable}"
                if default:
                    column_def += f" {default}"
                column_defs.append(column_def)
            
            create_table_sql += ",\n".join(column_defs)
            create_table_sql += "\n);\n"
            file.write(create_table_sql)
            
            # Экспортируем данные
            cursor.execute(f"SELECT * FROM {table_name}")
            rows = cursor.fetchall()
            
            if rows:
                file.write(f"\n-- Data for table {table_name}\n")
                
                for row in rows:
                    values = []
                    for col in columns:
                        col_name = col['column_name']
                        value = row[col_name]
                        
                        if value is None:
                            values.append("NULL")
                        elif isinstance(value, (int, float)):
                            values.append(str(value))
                        elif isinstance(value, bool):
                            values.append(str(value).lower())
                        else:
                            # Экранируем строки
                            escaped_value = str(value).replace("'", "''")
                            values.append(f"'{escaped_value}'")
                    
                    insert_sql = f"INSERT INTO {table_name} ({', '.join(col['column_name'] for col in columns)}) VALUES ({', '.join(values)});\n"
                    file.write(insert_sql)
            
            file.write(f"\n-- End of table {table_name}\n\n")
            
        except Exception as e:
            logger.error(f"Error exporting table {table_name}: {e}")
            file.write(f"\n-- Error exporting table {table_name}: {e}\n\n")
    
    def _export_indexes(self, cursor, file) -> None:
        """Экспорт индексов."""
        try:
            file.write("-- Indexes\n")
            cursor.execute("""
                SELECT indexname, tablename, indexdef
                FROM pg_indexes 
                WHERE schemaname = 'public'
                ORDER BY tablename, indexname
            """)
            
            for row in cursor.fetchall():
                if not row['indexname'].endswith('_pkey'):  # Пропускаем primary key
                    file.write(f"{row['indexdef']};\n")
            
            file.write("\n")
        except Exception as e:
            logger.error(f"Error exporting indexes: {e}")
    
    def _export_sequences(self, cursor, file) -> None:
        """Экспорт последовательностей."""
        try:
            file.write("-- Sequences\n")
            cursor.execute("""
                SELECT sequence_name, start_value, increment_by, last_value
                FROM information_schema.sequences 
                WHERE sequence_schema = 'public'
            """)
            
            for row in cursor.fetchall():
                file.write(f"ALTER SEQUENCE {row['sequence_name']} RESTART WITH {row['last_value']};\n")
            
            file.write("\n")
        except Exception as e:
            logger.error(f"Error exporting sequences: {e}")
    
    def _export_constraints(self, cursor, file) -> None:
        """Экспорт ограничений (foreign keys, unique, check)."""
        try:
            file.write("-- Constraints\n")
            
            # Foreign keys
            cursor.execute("""
                SELECT 
                    tc.table_name, 
                    tc.constraint_name, 
                    tc.constraint_type,
                    kcu.column_name,
                    ccu.table_name AS foreign_table_name,
                    ccu.column_name AS foreign_column_name
                FROM information_schema.table_constraints AS tc 
                JOIN information_schema.key_column_usage AS kcu
                    ON tc.constraint_name = kcu.constraint_name
                JOIN information_schema.constraint_column_usage AS ccu
                    ON ccu.constraint_name = tc.constraint_name
                WHERE tc.constraint_type = 'FOREIGN KEY' 
                AND tc.table_schema = 'public'
            """)
            
            for row in cursor.fetchall():
                fk_sql = f"ALTER TABLE {row['table_name']} ADD CONSTRAINT {row['constraint_name']} "
                fk_sql += f"FOREIGN KEY ({row['column_name']}) "
                fk_sql += f"REFERENCES {row['foreign_table_name']}({row['foreign_column_name']});\n"
                file.write(fk_sql)
            
            file.write("\n")
        except Exception as e:
            logger.error(f"Error exporting constraints: {e}")
    
    def _compress_file(self, source_path: Path, target_path: Path) -> None:
        """Сжатие файла."""
        try:
            with open(source_path, 'rb') as source:
                with gzip.open(target_path, 'wb') as target:
                    target.writelines(source)
            logger.info(f"File compressed: {target_path}")
        except Exception as e:
            logger.error(f"Compression failed: {e}")
            raise
    
    def restore_backup(self, backup_path: str) -> Dict[str, Any]:
        """
        Восстановление из резервной копии.
        
        Args:
            backup_path: Путь к файлу бэкапа
            
        Returns:
            Dict: Результат восстановления
        """
        try:
            backup_file = Path(backup_path)
            
            if not backup_file.exists():
                raise Exception("Файл резервной копии не найден")
            
            # Распаковываем если сжат
            if backup_file.suffix == '.gz':
                # Создаем временный файл в папке backups для распакованного SQL
                import uuid
                temp_sql_path = self.backup_dir / f"temp_unpack_{uuid.uuid4().hex}.sql"
                
                try:
                    logger.info(f"Unpacking .gz file: {backup_file}")
                    with gzip.open(backup_file, 'rb') as gz_file:
                        with open(temp_sql_path, 'wb') as sql_file:
                            sql_file.write(gz_file.read())
                    logger.info(f"Unpacked to: {temp_sql_path}")
                    
                    logger.info("Starting SQL restore execution...")
                    result = self._execute_sql_restore(str(temp_sql_path))
                    logger.info("SQL restore execution completed")
                finally:
                    # Удаляем временный распакованный файл
                    if temp_sql_path.exists():
                        temp_sql_path.unlink()
            else:
                result = self._execute_sql_restore(str(backup_file))
            
            logger.info("Backup restored successfully")
            return {
                'status': 'success',
                'message': 'База данных восстановлена успешно',
                'backup_file': backup_path
            }
            
        except Exception as e:
            logger.error(f"Backup restoration failed: {e}")
            raise Exception(f"Ошибка восстановления: {str(e)}")
    
    def _execute_sql_restore(self, sql_file_path: str) -> None:
        """Выполнение SQL восстановления."""
        try:
            logger.info(f"Reading SQL file: {sql_file_path}")
            with open(sql_file_path, 'r', encoding='utf-8') as f:
                sql_content = f.read()
            
            logger.info(f"SQL content length: {len(sql_content)} characters")
            logger.info("Connecting to database...")
            
            # Добавляем таймаут для подключения
            db_config_with_timeout = self.db_config.copy()
            db_config_with_timeout['connect_timeout'] = 30
            
            with psycopg2.connect(**db_config_with_timeout) as conn:
                logger.info("Database connected, executing SQL...")
                
                with conn.cursor() as cursor:
                    # Увеличиваем таймаут для выполнения запросов до 30 минут для больших бекапов
                    cursor.execute("SET statement_timeout = '1800s';")  # 30 минут
                    logger.info("Executing large SQL backup, this may take several minutes...")
                    cursor.execute(sql_content)
                    conn.commit()
                    logger.info("SQL executed successfully")
        except Exception as e:
            logger.error(f"SQL restore execution failed: {e}")
            raise
    
    def get_backup_size(self, backup_path: str) -> str:
        """Получение размера файла бэкапа."""
        try:
            file_path = Path(backup_path)
            if file_path.exists():
                size_bytes = file_path.stat().st_size
                if size_bytes < 1024:
                    return f"{size_bytes} B"
                elif size_bytes < 1024 * 1024:
                    return f"{size_bytes / 1024:.1f} KB"
                else:
                    return f"{size_bytes / (1024 * 1024):.1f} MB"
            return "Unknown"
        except Exception as e:
            logger.error(f"Error getting backup size: {e}")
            return "Error"
    
    def cleanup_old_backups(self, retention_count: int = 7):
        """
        Очистка старых резервных копий.
        
        Args:
            retention_count: Количество резервных копий для хранения
        """
        try:
            # Получаем все файлы бекапов
            backup_files = []
            for pattern in ['backup_*.sql', 'backup_*.sql.gz']:
                backup_files.extend(self.backup_dir.glob(pattern))
            
            # Сортируем по времени создания (новые первыми)
            backup_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
            
            # Удаляем файлы сверх лимита
            files_to_delete = backup_files[retention_count:]
            
            for backup_file in files_to_delete:
                try:
                    backup_file.unlink()
                    logger.info(f"Deleted old backup: {backup_file.name}")
                except Exception as e:
                    logger.error(f"Failed to delete old backup {backup_file.name}: {e}")
            
            if files_to_delete:
                logger.info(f"Cleaned up {len(files_to_delete)} old backup files")
            else:
                logger.debug(f"No old backups to clean up (keeping {retention_count} files)")
                
        except Exception as e:
            logger.error(f"Backup cleanup failed: {e}")
            raise Exception(f"Ошибка очистки старых бекапов: {str(e)}")
    
    def list_backups(self) -> List[Dict[str, Any]]:
        """Получение списка всех бэкапов."""
        try:
            backups = []
            for backup_file in self.backup_dir.glob("backup_*.sql*"):
                stat = backup_file.stat()
                backups.append({
                    'filename': backup_file.name,
                    'path': str(backup_file),
                    'size': self.get_backup_size(str(backup_file)),
                    'created_at': datetime.fromtimestamp(stat.st_mtime).isoformat(),
                    'is_compressed': backup_file.suffix == '.gz'
                })
            
            # Сортируем по дате создания (новые первыми)
            backups.sort(key=lambda x: x['created_at'], reverse=True)
            return backups
            
        except Exception as e:
            logger.error(f"Error listing backups: {e}")
            return []
    
    def delete_backup(self, backup_path: str) -> bool:
        """Удаление бэкапа."""
        try:
            backup_file = Path(backup_path)
            if backup_file.exists():
                backup_file.unlink()
                logger.info(f"Backup deleted: {backup_path}")
                return True
            return False
        except Exception as e:
            logger.error(f"Error deleting backup: {e}")
            return False